unlink('R/Viikkotehtävä2_cache', recursive = TRUE)
install.packages("mosaic")
3+4
knitr::opts_chunk$set(echo = TRUE)
plot(1:10, 1:10)
knitr::opts_chunk$set(echo = TRUE)
summary(cars)
rm(list=ls())
library(rstudioapi)
my_work_dir = rstudioapi::getActiveDocumentContext()$path
my_work_dir = gsub("\\\\", "/", my_work_dir)
pathParts = strsplit(my_work_dir,'/')[[1]]
my_work_dir = paste(pathParts[1:length(pathParts)-1],sep='', collapse = "/")
setwd(my_work_dir)
libPath = paste(my_work_dir,'/','my_libs', sep="")
if (!file.exists(libPath))
{
dir.create(libPath, showWarnings = FALSE)
}
dataPath = paste(my_work_dir,'/','Data', sep="")
if (!file.exists(dataPath))
{
dir.create(dataPath, showWarnings = FALSE)
}
twitterDataPath = paste(my_work_dir,'/','Data','/','Twitter', sep="")
if (!file.exists(twitterDataPath))
{
dir.create(twitterDataPath, showWarnings = FALSE)
}
stackDataPath = paste(my_work_dir,'/','Data','/','Stack', sep="")
if (!file.exists(stackDataPath))
{
dir.create(stackDataPath, showWarnings = FALSE)
}
scopusDataPath = paste(my_work_dir,'/','Data','/','Scopus', sep="")
if (!file.exists(scopusDataPath))
{
dir.create(scopusDataPath, showWarnings = FALSE)
}
if (libPath %in% .libPaths() == FALSE)
{
.libPaths(c(libPath,.libPaths()))
}
sc_api_key = "5f8efe2dcfaa441f24530b2f09123a55"
so_api_key = "kN*rOg9tz*1FkrJPeQiGwg(("
tw_api_key = ""
my_data_dir = 'data'
getoldtweets_path = paste(my_work_dir,"/GetOldTweets-java-master", sep="")
library("stopwords")
my_stopwords = c(stopwords::stopwords(language = "en", source = "snowball"), "based", "paper", "can")
library(text2vec)
library(tm)
library(magrittr)
library(LDAvis)
my_data_dir='data'
#May take a while - wait patiently
my_file = "my_Scopus_testautomation3_data.RData"
#draw_my_IAMap = function(my_file) {
print(paste("Interactive LDA Cluster, my_file: ", my_file))
my_temp_file = paste(my_data_dir, "/", sep="")
my_temp_file = paste(my_temp_file, my_file, sep="")
load(my_temp_file)
my_text <- paste (my_articles$Title, my_articles$Abstract_clean)
#Here we use text2vec package as it contains very fast LDA clustering implementation
#Note there is some overlap with between text2vec, tm, and topicmodels pacakges
my_tokens = my_text %>% tolower %>% word_tokenizer
it = itoken(my_tokens)
#my_stopwords = c(stopwords::stopwords(language = "en", source = "snowball"),"myStopword1", "myStopword2")
#Remove stopwords
my_vocab <- create_vocabulary(it, stopwords=my_stopwords)
#Limit vocabulary to 40000 terms.
#Each word is only allowed to appear in 50% of the documents
#vocab <- prune_vocabulary(vocab, max_number_of_terms=40000, doc_proportion_max=0.5)
my_vocab <- prune_vocabulary(my_vocab,
term_count_min=10,
vocab_term_max=40000,
doc_proportion_max=0.5)
View(my_vocab)
vectorizer=vocab_vectorizer(my_vocab)
dtm = create_dtm(it, vectorizer,'dgCMatrix')
#LDA model with 20 topics to provide an overview
#lda_model = LatentDirichletAllocation$new(n_topics=20, vocabulary=my_vocab)
lda_model = LatentDirichletAllocation$new(n_topics=20)
#We run 200 iterations
doc_topic_distr = lda_model$fit_transform(dtm, n_iter=200, check_convergence_every_n=5)
#Run LDAvis visualisation if needed (make sure LDAvis package installed)
#Be patient this might take while before anything is plotted. See viewer tap on the right
if (!all(lda_model$topic_word_distribution == 0)) {
lda_model$plot()
} else {
print("No Data!")
}
print("Finished Interactive LDA Cluster")
lda_model = LatentDirichletAllocation$new(n_topics=10)
#We run 200 iterations
doc_topic_distr = lda_model$fit_transform(dtm, n_iter=200, check_convergence_every_n=5)
#Run LDAvis visualisation if needed (make sure LDAvis package installed)
#Be patient this might take while before anything is plotted. See viewer tap on the right
if (!all(lda_model$topic_word_distribution == 0)) {
lda_model$plot()
} else {
print("No Data!")
}
print("Finished Interactive LDA Cluster")
#install.packages("tm", dependencies = TRUE)
#install.packages("NLP", dependencies = TRUE)
#install.packages("magrittr", dependencies = TRUE)
#install.packages("slam", dependencies = TRUE)
#install.packages("Rmpfr", dependencies = TRUE)
library(tm)
library("magrittr")
library("text2vec")
library("tokenizers")
install.packages("tokenizers",dependencies = TRUE)
library("tokenizers")
#Set the file to be analyzed, e.g.
my_file = "my_Scopus_ta_data.RData"
my_temp_file = paste(my_data_dir, "/", sep="")
my_temp_file = paste(my_temp_file, my_file, sep="")
#Articles with NA dates cause false analysis later kick them out
my_articles <- my_articles[which(!is.na(my_articles$Date)),]
my_text <- paste (my_articles$Title, my_articles$Abstract_clean)
removeSpecialChars <- function(x) gsub("[^a-zA-Z ]","",x)
my_text <- removeSpecialChars(my_text)
my_text <- removeWords(my_text, my_stopwords)
my_articles$Clean_Text <- my_text
#-----------------------------------------------------------------------------------
#Create first LDA model. We select 80% for model creation remaining is for testing
#See tutorial for more details http://text2vec.org/topic_modeling.html#latent_dirichlet_allocation
#model goodness
sample <- sample.int(n = nrow(my_articles), size = floor(.80*nrow(my_articles)), replace = F)
#create tokens
tokens = my_articles$Clean_Text [sample] %>%  tokenize_words (strip_numeric = TRUE)
it <- itoken(tokens, progressbar = FALSE)
#Here we create the vocabulary. Term must appear in min 10 documents (you might want to edit this)
#If term appears in more than 30% documents we remove because it is too frequent (you might want to edit this as well)
v = create_vocabulary(it) %>% prune_vocabulary(term_count_min = 10, doc_proportion_max = 0.3)
vectorizer = vocab_vectorizer(v)
#create document-term matrix
dtm = create_dtm(it, vectorizer, type = "dgTMatrix")
# we create 10 topics
lda_model = LDA$new(n_topics = 10, doc_topic_prior = 0.1, topic_word_prior = 0.01)
doc_topic_distr = lda_model$fit_transform(x = dtm, n_iter = 1000,
convergence_tol = 0.001, n_check_convergence = 25,
#convergence_tol = 0.01, n_check_convergence = 25,
progressbar = FALSE, verbose=FALSE)
#apply to training set
new_dtm = itoken(my_articles$Clean_Text[-sample], tolower, word_tokenizer) %>%
create_dtm(vectorizer, type = "dgTMatrix")
new_doc_topic_distr = lda_model$transform(new_dtm)
perpperplexity_score <- perplexity(new_dtm, topic_word_distribution = lda_model$topic_word_distribution, doc_topic_distribution = new_doc_topic_distr)
#how good is our model
#Try playin with n_topics, doc_topic_prior, topic_word_prior to see how to get better
perpperplexity_score
# we create 10 topics
lda_model = LDA$new(n_topics = 20, doc_topic_prior = 0.1, topic_word_prior = 0.01)
doc_topic_distr = lda_model$fit_transform(x = dtm, n_iter = 1000,
convergence_tol = 0.001, n_check_convergence = 25,
#convergence_tol = 0.01, n_check_convergence = 25,
progressbar = FALSE, verbose=FALSE)
#apply to training set
new_dtm = itoken(my_articles$Clean_Text[-sample], tolower, word_tokenizer) %>%
create_dtm(vectorizer, type = "dgTMatrix")
new_doc_topic_distr = lda_model$transform(new_dtm)
perpperplexity_score <- perplexity(new_dtm, topic_word_distribution = lda_model$topic_word_distribution, doc_topic_distribution = new_doc_topic_distr)
#how good is our model
#Try playin with n_topics, doc_topic_prior, topic_word_prior to see how to get better
perpperplexity_score
# we create 10 topics
lda_model = LDA$new(n_topics = 90, doc_topic_prior = 0.1, topic_word_prior = 0.01)
doc_topic_distr = lda_model$fit_transform(x = dtm, n_iter = 1000,
convergence_tol = 0.001, n_check_convergence = 25,
#convergence_tol = 0.01, n_check_convergence = 25,
progressbar = FALSE, verbose=FALSE)
#apply to training set
new_dtm = itoken(my_articles$Clean_Text[-sample], tolower, word_tokenizer) %>%
create_dtm(vectorizer, type = "dgTMatrix")
new_doc_topic_distr = lda_model$transform(new_dtm)
perpperplexity_score <- perplexity(new_dtm, topic_word_distribution = lda_model$topic_word_distribution, doc_topic_distribution = new_doc_topic_distr)
#how good is our model
#Try playin with n_topics, doc_topic_prior, topic_word_prior to see how to get better
perpperplexity_score
#Lets investigate our topics
lda_model$get_top_words(n = 7, topic_number = c(1:10), lambda = 1)
#Lambda setting highlight more topic specific but less probable words over all. Observe the difference
lda_model$get_top_words(n = 7, topic_number = c(1:10), lambda = 0.3)
library(DEoptim)
install.packages("DEoptim",dependencies = TRUE)
library(DEoptim)
#Search space needs to be defined topics are between 10-500 and hyberparameters are between 0 and 1
lower <- c(10, 0, 0)
higher <- c(500, 1, 0.3)
#here we start the search with 30 item population
#reduce / increase itermax and NP if too slow or fast
##The Deoptim package is really picky and may require R-studio restart to work correcly. If you see no print then it is not working
#NP should be 30 3 parameter ten times for each (3x10)
DEoptim(optimalLda, lower, higher, DEoptim.control(strategy = 2, itermax = 10, NP = 10, CR = 0.5, F = 0.8))
